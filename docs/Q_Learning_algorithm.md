## ğŸ¤– 2. Q-Learning (PekiÅŸtirmeli Ã–ÄŸrenme)

### ğŸ“Œ Nedir?
Q-Learning, makine Ã¶ÄŸrenmesinin "PekiÅŸtirmeli Ã–ÄŸrenme" (Reinforcement Learning) alt dalÄ±na ait bir algoritmadÄ±r. Bir Ã¶ÄŸretmenin Ã¶ÄŸrenciye doÄŸru yaptÄ±ÄŸÄ± ÅŸeyler iÃ§in Ã¶dÃ¼l, yanlÄ±ÅŸ yaptÄ±ÄŸÄ± ÅŸeyler iÃ§in ceza vermesi mantÄ±ÄŸÄ±na dayanÄ±r.

Ortamda dolaÅŸan bir **"Ajan" (Agent)** vardÄ±r. Bu ajan deneme-yanÄ±lma yoluyla hangi hareketlerin ona en Ã§ok puanÄ± (Ã¶dÃ¼lÃ¼) kazandÄ±rdÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenir ve bu tecrÃ¼belerini bir hafÄ±za tablosuna (**Q-Table**) kaydeder.

### ğŸš€ Bu Projede Neden ve NasÄ±l KullandÄ±k?
QoS Routing dinamik bir sÃ¼reÃ§tir. AÄŸ trafiÄŸi anlÄ±k deÄŸiÅŸebilir. Q-Learning, aÄŸÄ±n iÃ§inde sÃ¼rekli gezinen paketler (ajanlar) sayesinde hangi yolun tÄ±kalÄ±, hangi yolun hÄ±zlÄ± olduÄŸunu zamanla Ã¶ÄŸrenir.



### ğŸ§  Temel Terimler ve Projedeki KarÅŸÄ±lÄ±klarÄ±

| Terim | Projedeki (QoS Routing) KarÅŸÄ±lÄ±ÄŸÄ± | AÃ§Ä±klama |
| :--- | :--- | :--- |
| **Agent (Ajan)** | Veri Paketi / YÃ¶nlendirici YazÄ±lÄ±mÄ± | AÄŸ Ã¼zerinde yolunu bulmaya Ã§alÄ±ÅŸan akÄ±llÄ± birim. |
| **State (Durum)** | Mevcut Router (DÃ¼ÄŸÃ¼m) | AjanÄ±n o an bulunduÄŸu konum (Ã–rn: Router A). |
| **Action (Eylem)** | Bir sonraki Router'a geÃ§iÅŸ | KomÅŸu dÃ¼ÄŸÃ¼mlerden hangisine gidileceÄŸi kararÄ±. |
| **Reward (Ã–dÃ¼l)** | BaÄŸlantÄ± Kalitesi | Gidilen yol hÄ±zlÄ±ysa (+), yavaÅŸ veya kopuksa (-) puan verilir. |
| **Q-Table** | YÃ¶nlendirme Tablosu (Routing Table) | Hangi dÃ¼ÄŸÃ¼mden nereye gidilirse ne kadar Ã¶dÃ¼l alÄ±nacaÄŸÄ±nÄ±n tutulduÄŸu hafÄ±za matrisi. |

---

### âš™ï¸ Ã‡alÄ±ÅŸma MantÄ±ÄŸÄ±

1.  **KeÅŸif (Exploration):** Ajan baÅŸta Ã§evreyi bilmediÄŸi iÃ§in rastgele yollara girer.
2.  **Eylem ve SonuÃ§:** Ajan bir dÃ¼ÄŸÃ¼mden diÄŸerine geÃ§er (Ã–rn: A -> B).
3.  **Ã–dÃ¼l/Ceza:**
    * EÄŸer B dÃ¼ÄŸÃ¼mÃ¼ne giden hat boÅŸ ve hÄ±zlÄ±ysa (DÃ¼ÅŸÃ¼k Gecikme), Ajan **pozitif Ã¶dÃ¼l** alÄ±r.
    * EÄŸer hat tÄ±kalÄ±ysa, Ajan **negatif Ã¶dÃ¼l (ceza)** alÄ±r.
4.  **Q-Tablosunu GÃ¼ncelleme:** Ajan, "A'dan B'ye gitmek iyi bir fikir" veya "kÃ¶tÃ¼ bir fikir" bilgisini matematiksel olarak Q-Tablosuna yazar.
5.  **SÃ¶mÃ¼rÃ¼ (Exploitation):** Ä°lerleyen turlarda Ajan artÄ±k rastgele gitmez, Q-Tablosuna bakÄ±p en yÃ¼ksek puanlÄ± yolu seÃ§er.


---

### ğŸ§  ALGORÄ°TMA MÄ°MARÄ°SÄ°: Q-LEARNING


Algoritma, **Epsilon-Greedy** politikasÄ± ile Ã§alÄ±ÅŸÄ±r. Bu sayede ajan:
1.  **KeÅŸif (Exploration):** BilmediÄŸi yeni yollarÄ± dener.
2.  **SÃ¶mÃ¼rÃ¼ (Exploitation):** Daha Ã¶nce Ã¶ÄŸrendiÄŸi en iyi yollarÄ± kullanÄ±r.

### âš™ï¸ HÄ°PER-PARAMETRELER
| PARAMETRE | DEÄER | AÃ‡IKLAMA |
| :--- | :--- | :--- |
| **Ã–ÄŸrenme OranÄ± ($\alpha$)** | 0.1 | Yeni bilginin ne kadar baskÄ±n olacaÄŸÄ±nÄ± belirler. |
| **Ä°ndirim FaktÃ¶rÃ¼ ($\gamma$)** | 0.9 | Gelecekteki Ã¶dÃ¼llerin bugÃ¼nkÃ¼ deÄŸerini belirler. |
| **Epsilon ($\epsilon$)** | 1.0 | BaÅŸlangÄ±Ã§taki rastgele hareket oranÄ±dÄ±r. |
| **Epizot SayÄ±sÄ±** | 800 | Toplam eÄŸitim deneme sayÄ±sÄ±dÄ±r. |

---

## âš–ï¸ Ã–DÃœL VE CEZA SÄ°STEMÄ° (REWARD FUNCTION)

AjanÄ±n doÄŸru rotayÄ± bulmasÄ± iÃ§in aÅŸaÄŸÄ±daki puanlama mekanizmasÄ± kurgulanmÄ±ÅŸtÄ±r:

* **HEDEFE ULAÅMA:** $+ (Maliyet \times 2)$ (En yÃ¼ksek Ã¶dÃ¼l).
* **DÃ–NGÃœ CEZASI:** $-1000$ (AynÄ± dÃ¼ÄŸÃ¼me tekrar girilirse).
* **BANT GENÄ°ÅLÄ°ÄÄ° Ä°HLALÄ°:** $-500$ (Talep karÅŸÄ±lanmazsa).
* **ADIM CEZASI:** $-1$ (Yolun gereksiz uzamasÄ±nÄ± engeller).

--- 

### ğŸ“Š Ã–zet
* **âœ… AvantajÄ±:** Ã–ÄŸrenebilen bir yapÄ±dÄ±r. AÄŸÄ±n durumu deÄŸiÅŸtikÃ§e (bir hat koptuÄŸunda), ajan ceza alacaÄŸÄ± iÃ§in o yolu kullanmayÄ± bÄ±rakÄ±p alternatif yollarÄ± kendi kendine Ã¶ÄŸrenir.
* **âŒ DezavantajÄ±:** BaÅŸlangÄ±Ã§ta "Ã¶ÄŸrenme sÃ¼reci" olduÄŸu iÃ§in optimum yolu bulmasÄ± zaman alÄ±r. BÃ¼yÃ¼k aÄŸlarda Q-Tablosu (hafÄ±za) Ã§ok yer kaplayabilir.